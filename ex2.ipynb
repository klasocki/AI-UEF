{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is overfitting?\n",
    "\n",
    "Overfitting is the situation when a model tries too hard to fit the training data or even starts to memorize it, thus losing the ability to generalise. Performance on training data is still good, but performance on test data drops\n",
    "2. Explain how we can determine the best number of training iterations to avoid overfitting.\n",
    "\n",
    "We can split our dataset into three sets instead of two - the training, validation and testing set. After each iteration (or epoch), we test the model on the validation set. When training is finished, we pick the model with the best result on the validation set, and test it on the testing set to obtain actual results (validation results could be biased, because we could have simply chosen the model that best reflects the validation set)\n",
    "3. What are the methods used to prevent a neural network from overfitting? \n",
    "\n",
    "Other than picking the model based on validation results:\n",
    "* Using dropout to randomly switch off some neurons\n",
    "* Use a regularization technique to prevent the model from getting too complex\n",
    "* Simplify the model manually - reduce number of neurons and/or hidden layers\n",
    "\n",
    "These techniques could help stop the model from memorizing the training data\n",
    "\n",
    "4. Is it possible to represent a XOR Boolean function with a single layer perceptron? Why/Why not? \n",
    "\n",
    "It's not possible, because XOR function is not linearly-separable \n",
    "5. What is the advantage of multi-layer neural networks over single layer neural networks?\n",
    "\n",
    "Multi-layer networks can learn arbitrarily complex functions, thanks to combining functions learned by the previous layer. Sigle layer networks can only learn linear separable functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating input samples for each function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T16:37:14.466798Z",
     "start_time": "2019-09-15T16:37:14.462644Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T17:19:36.180132Z",
     "start_time": "2019-09-15T17:19:36.169493Z"
    }
   },
   "outputs": [],
   "source": [
    "outputs = {}\n",
    "inputs = list(product([False, True], repeat=2))\n",
    "outputs['and'] = np.array([x and y for x,y in inputs], dtype=float)\n",
    "outputs['or'] = np.array([x or y for x,y in inputs], dtype=float)\n",
    "outputs['nor'] = np.array([not (x or y) for x,y in inputs], dtype=float)\n",
    "inputs = np.array(inputs, dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the gradient by hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss = (output - expected) ^ 2 =\n",
    "\n",
    "= (X * W + b  - expected) ^ 2\n",
    "\n",
    "dW/dloss:\n",
    "\n",
    "2(X * W + b - expected) * X\n",
    "\n",
    "db/dloss:\n",
    "\n",
    "2(X * W + b - expected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T17:27:33.173575Z",
     "start_time": "2019-09-15T17:27:33.153000Z"
    }
   },
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, input_size=2):\n",
    "        self.input_size = input_size\n",
    "        self.weigths = np.random.rand(input_size)\n",
    "        self.bias = np.random.rand(1)\n",
    "    \n",
    "    def forward(self, X: np.array) -> np.array:\n",
    "        return np.dot(X, self.weigths) + self.bias\n",
    "    \n",
    "    def train(self, X: np.array, y: np.array, epochs: int, learning_rate: float) -> None:\n",
    "        for epoch in range(epochs):\n",
    "            print('a-\\n\\nEpoch:', epoch, '\\n\\n--------------------')\n",
    "            for inp, expected in zip(X, y):\n",
    "                output = self.forward(inp)\n",
    "                loss = (output - expected) ** 2\n",
    "                print('Loss:', loss.item())\n",
    "                W_grad = 2 * (output - expected) * inp\n",
    "                b_grad = 2 * (output - expected)\n",
    "                self.weigths -= W_grad * learning_rate\n",
    "                self.bias -= b_grad * learning_rate\n",
    "    \n",
    "    def predict(self, X: np.array, threshold=.5) -> bool:\n",
    "        return self.forward(X).item() > threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T17:30:50.474589Z",
     "start_time": "2019-09-15T17:30:50.326615Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "\n",
      " and \n",
      "\n",
      "--------------------------\n",
      "---------------\n",
      "\n",
      "Epoch: 0 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.3721609037310162\n",
      "Loss: 0.3240473198262275\n",
      "Loss: 0.3621477052696172\n",
      "Loss: 0.3083457220114267\n",
      "---------------\n",
      "\n",
      "Epoch: 1 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.23762695200629422\n",
      "Loss: 0.2093367235987494\n",
      "Loss: 0.24799117346109292\n",
      "Loss: 0.3950792314532717\n",
      "---------------\n",
      "\n",
      "Epoch: 2 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.1648573082258114\n",
      "Loss: 0.16107055540127674\n",
      "Loss: 0.1965717552458642\n",
      "Loss: 0.42188914641743486\n",
      "---------------\n",
      "\n",
      "Epoch: 3 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.11965137680737786\n",
      "Loss: 0.13841948037270135\n",
      "Loss: 0.17040538138113417\n",
      "Loss: 0.4176151910276388\n",
      "---------------\n",
      "\n",
      "Epoch: 4 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.08847896477338583\n",
      "Loss: 0.12663552397279892\n",
      "Loss: 0.1553593468003593\n",
      "Loss: 0.3995637551726372\n",
      "---------------\n",
      "\n",
      "Epoch: 5 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.06549414823697795\n",
      "Loss: 0.11978640049559461\n",
      "Loss: 0.14557682742137404\n",
      "Loss: 0.37650326743983953\n",
      "---------------\n",
      "\n",
      "Epoch: 6 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.0479266578781385\n",
      "Loss: 0.1152971977723692\n",
      "Loss: 0.13846464437630565\n",
      "Loss: 0.35253164071986887\n",
      "---------------\n",
      "\n",
      "Epoch: 7 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.03431293792030309\n",
      "Loss: 0.11199576882619869\n",
      "Loss: 0.13282083669653869\n",
      "Loss: 0.32943593695622714\n",
      "---------------\n",
      "\n",
      "Epoch: 8 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.023777613444716596\n",
      "Loss: 0.10933245582949545\n",
      "Loss: 0.12806686792660255\n",
      "Loss: 0.30791328012073393\n",
      "---------------\n",
      "\n",
      "Epoch: 9 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.015729666334001607\n",
      "Loss: 0.10704368335861833\n",
      "Loss: 0.12391244307658304\n",
      "Loss: 0.28816368433586453\n",
      "---------------\n",
      "\n",
      "Epoch: 10 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.009729518037351361\n",
      "Loss: 0.10499996182188989\n",
      "Loss: 0.12020354162602034\n",
      "Loss: 0.2701693249233121\n",
      "---------------\n",
      "\n",
      "Epoch: 11 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.005428480448256346\n",
      "Loss: 0.10313539953324533\n",
      "Loss: 0.11685196462839699\n",
      "Loss: 0.25382367124122446\n",
      "---------------\n",
      "\n",
      "Epoch: 12 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.0025394543501324972\n",
      "Loss: 0.10141455554851635\n",
      "Loss: 0.11380219783371022\n",
      "Loss: 0.23899014174093938\n",
      "---------------\n",
      "\n",
      "Epoch: 13 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.0008214003970084457\n",
      "Loss: 0.09981673313272715\n",
      "Loss: 0.11101563744011265\n",
      "Loss: 0.22552803934501287\n",
      "---------------\n",
      "\n",
      "Epoch: 14 \n",
      "\n",
      "--------------------\n",
      "Loss: 7.007485238339676e-05\n",
      "Loss: 0.09832850221661059\n",
      "Loss: 0.10846297520573656\n",
      "Loss: 0.21330342891970047\n",
      "---------------\n",
      "\n",
      "Epoch: 15 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.00011177446068451846\n",
      "Loss: 0.09694012535302182\n",
      "Loss: 0.10612044671931602\n",
      "Loss: 0.20219316236857543\n",
      "---------------\n",
      "\n",
      "Epoch: 16 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.0007986591581387771\n",
      "Loss: 0.09564384083937051\n",
      "Loss: 0.10396792286342436\n",
      "Loss: 0.19208584654286218\n",
      "---------------\n",
      "\n",
      "Epoch: 17 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.002005006491569834\n",
      "Loss: 0.09443303109391722\n",
      "Loss: 0.10198788991094954\n",
      "Loss: 0.18288150084662935\n",
      "---------------\n",
      "\n",
      "Epoch: 18 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.003624090929378359\n",
      "Loss: 0.0933018136825172\n",
      "Loss: 0.10016486504570359\n",
      "Loss: 0.17449070225445487\n",
      "---------------\n",
      "\n",
      "Epoch: 19 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.005565529328830578\n",
      "Loss: 0.09224483453796031\n",
      "Loss: 0.09848503089000066\n",
      "Loss: 0.16683357648831945\n",
      "---------------\n",
      "\n",
      "Epoch: 20 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.0077529999472953914\n",
      "Loss: 0.09125715823102465\n",
      "Loss: 0.09693598481586108\n",
      "Loss: 0.15983879169751042\n",
      "---------------\n",
      "\n",
      "Epoch: 21 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.010122273351495577\n",
      "Loss: 0.09033420512323644\n",
      "Loss: 0.09550655216185797\n",
      "Loss: 0.153442618214218\n",
      "---------------\n",
      "\n",
      "Epoch: 22 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.012619509361287114\n",
      "Loss: 0.08947171145055166\n",
      "Loss: 0.09418663796304667\n",
      "Loss: 0.14758807594894932\n",
      "---------------\n",
      "\n",
      "Epoch: 23 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.015199783257452784\n",
      "Loss: 0.08866570089627974\n",
      "Loss: 0.09296710406579341\n",
      "Loss: 0.14222417243784663\n",
      "---------------\n",
      "\n",
      "Epoch: 24 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.017825810493129226\n",
      "Loss: 0.08791246217951543\n",
      "Loss: 0.09183966447106548\n",
      "Loss: 0.13730522679108545\n",
      "------------------------\n",
      "\n",
      " or \n",
      "\n",
      "--------------------------\n",
      "---------------\n",
      "\n",
      "Epoch: 0 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.9642720593199217\n",
      "Loss: 0.07571199496507584\n",
      "Loss: 0.22115158580241237\n",
      "Loss: 0.547718659782273\n",
      "---------------\n",
      "\n",
      "Epoch: 1 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.5405566994712306\n",
      "Loss: 0.0023463222313635373\n",
      "Loss: 0.025446421214027324\n",
      "Loss: 0.17835203695702473\n",
      "---------------\n",
      "\n",
      "Epoch: 2 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.37010584974678484\n",
      "Loss: 0.04000113290441347\n",
      "Loss: 5.363583749352605e-06\n",
      "Loss: 0.07525325902801978\n",
      "---------------\n",
      "\n",
      "Epoch: 3 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.29145237288486747\n",
      "Loss: 0.0724066532393636\n",
      "Loss: 0.00641436751914219\n",
      "Loss: 0.04321196627511373\n",
      "---------------\n",
      "\n",
      "Epoch: 4 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.2500069328073717\n",
      "Loss: 0.08930210024144916\n",
      "Loss: 0.015816586175248226\n",
      "Loss: 0.03255550439959035\n",
      "---------------\n",
      "\n",
      "Epoch: 5 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.22507713881762267\n",
      "Loss: 0.09611230014360124\n",
      "Loss: 0.023451188033140287\n",
      "Loss: 0.02940924591207314\n",
      "---------------\n",
      "\n",
      "Epoch: 6 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.20807035799509807\n",
      "Loss: 0.09772818078454175\n",
      "Loss: 0.02929633343927402\n",
      "Loss: 0.029304131756750392\n",
      "---------------\n",
      "\n",
      "Epoch: 7 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.19517999244303638\n",
      "Loss: 0.09696495384061787\n",
      "Loss: 0.033931944973428066\n",
      "Loss: 0.030544417028662545\n",
      "---------------\n",
      "\n",
      "Epoch: 8 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.18463820429807218\n",
      "Loss: 0.09524409089771742\n",
      "Loss: 0.037801793784902366\n",
      "Loss: 0.03239201915327296\n",
      "---------------\n",
      "\n",
      "Epoch: 9 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.17558821303643346\n",
      "Loss: 0.09323817110473655\n",
      "Loss: 0.041170582804625086\n",
      "Loss: 0.03449643396165877\n",
      "---------------\n",
      "\n",
      "Epoch: 10 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.1675931889158403\n",
      "Loss: 0.09124837020387364\n",
      "Loss: 0.04418467761930945\n",
      "Loss: 0.03668349425364302\n",
      "---------------\n",
      "\n",
      "Epoch: 11 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.16041457640839915\n",
      "Loss: 0.08939876912423433\n",
      "Loss: 0.04692389780684089\n",
      "Loss: 0.038864914490045595\n",
      "---------------\n",
      "\n",
      "Epoch: 12 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.15390985238733407\n",
      "Loss: 0.08773136539990244\n",
      "Loss: 0.0494331345480339\n",
      "Loss: 0.04099573990118234\n",
      "---------------\n",
      "\n",
      "Epoch: 13 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.14798466918553516\n",
      "Loss: 0.08625138991627367\n",
      "Loss: 0.051739646569465815\n",
      "Loss: 0.043053317154848246\n",
      "---------------\n",
      "\n",
      "Epoch: 14 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.14257014840564228\n",
      "Loss: 0.08494857831217016\n",
      "Loss: 0.053862014988389384\n",
      "Loss: 0.04502665529886958\n",
      "---------------\n",
      "\n",
      "Epoch: 15 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.13761192059812655\n",
      "Loss: 0.08380699308896503\n",
      "Loss: 0.05581460360912333\n",
      "Loss: 0.04691097677739163\n",
      "---------------\n",
      "\n",
      "Epoch: 16 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.13306469400777132\n",
      "Loss: 0.0828094451510538\n",
      "Loss: 0.057609698082787236\n",
      "Loss: 0.0487048977361742\n",
      "---------------\n",
      "\n",
      "Epoch: 17 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.1288894491935033\n",
      "Loss: 0.08193938946289622\n",
      "Loss: 0.059258483662274215\n",
      "Loss: 0.05040894816807145\n",
      "---------------\n",
      "\n",
      "Epoch: 18 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.12505189682739093\n",
      "Loss: 0.08118165348305574\n",
      "Loss: 0.06077146026124968\n",
      "Loss: 0.05202477774744581\n",
      "---------------\n",
      "\n",
      "Epoch: 19 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.12152155582705813\n",
      "Loss: 0.08052263766928452\n",
      "Loss: 0.062158596297734195\n",
      "Loss: 0.05355471497999012\n",
      "---------------\n",
      "\n",
      "Epoch: 20 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.11827114655912041\n",
      "Loss: 0.07995028706156006\n",
      "Loss: 0.06342936943035649\n",
      "Loss: 0.055001510744843954\n",
      "---------------\n",
      "\n",
      "Epoch: 21 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.1152761526751145\n",
      "Loss: 0.0794539723296776\n",
      "Loss: 0.06459276471063097\n",
      "Loss: 0.05636818023345524\n",
      "---------------\n",
      "\n",
      "Epoch: 22 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.11251448014268858\n",
      "Loss: 0.07902434307772721\n",
      "Loss: 0.06565726216127407\n",
      "Loss: 0.05765789927560684\n",
      "---------------\n",
      "\n",
      "Epoch: 23 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.10996617763989668\n",
      "Loss: 0.07865318080989331\n",
      "Loss: 0.0666308270937822\n",
      "Loss: 0.05887393227295712\n",
      "---------------\n",
      "\n",
      "Epoch: 24 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.10761319952486165\n",
      "Loss: 0.07833326256414547\n",
      "Loss: 0.06752090768426582\n",
      "Loss: 0.06001957970354653\n",
      "------------------------\n",
      "\n",
      " nor \n",
      "\n",
      "--------------------------\n",
      "---------------\n",
      "\n",
      "Epoch: 0 \n",
      "\n",
      "--------------------\n",
      "Loss: 2.5389428779630275e-05\n",
      "Loss: 3.652287783309961\n",
      "Loss: 2.2665776532429494\n",
      "Loss: 3.720785895279088\n",
      "---------------\n",
      "\n",
      "Epoch: 1 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.2906174581774974\n",
      "Loss: 1.0950520958541836\n",
      "Loss: 0.5896550978814262\n",
      "Loss: 1.0842956521212\n",
      "---------------\n",
      "\n",
      "Epoch: 2 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.5940459777129415\n",
      "Loss: 0.39587280071971376\n",
      "Loss: 0.17657555544769155\n",
      "Loss: 0.35533803421487475\n",
      "---------------\n",
      "\n",
      "Epoch: 3 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.7365397807507558\n",
      "Loss: 0.18312230066491952\n",
      "Loss: 0.06758740859387918\n",
      "Loss: 0.13359961727554473\n",
      "---------------\n",
      "\n",
      "Epoch: 4 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.7704252501628241\n",
      "Loss: 0.1095716335242886\n",
      "Loss: 0.035929327624064374\n",
      "Loss: 0.057369620964875016\n",
      "---------------\n",
      "\n",
      "Epoch: 5 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.7499099956244739\n",
      "Loss: 0.08096941280675557\n",
      "Loss: 0.0262046715836554\n",
      "Loss: 0.027216775060118024\n",
      "---------------\n",
      "\n",
      "Epoch: 6 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.7064681273470743\n",
      "Loss: 0.06891138721981348\n",
      "Loss: 0.02381111978980492\n",
      "Loss: 0.013495596435015196\n",
      "---------------\n",
      "\n",
      "Epoch: 7 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.6557174292767378\n",
      "Loss: 0.06366500248294998\n",
      "Loss: 0.02432263692053218\n",
      "Loss: 0.006502887521409083\n",
      "---------------\n",
      "\n",
      "Epoch: 8 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.6047848123158351\n",
      "Loss: 0.06145416042988935\n",
      "Loss: 0.026119601067341673\n",
      "Loss: 0.002736674186159317\n",
      "---------------\n",
      "\n",
      "Epoch: 9 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.5566563160440876\n",
      "Loss: 0.06066605219714398\n",
      "Loss: 0.02849639591002913\n",
      "Loss: 0.0007955759449853\n",
      "---------------\n",
      "\n",
      "Epoch: 10 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.5123933853224558\n",
      "Loss: 0.06056689736227937\n",
      "Loss: 0.031108891632495028\n",
      "Loss: 4.664502462356003e-05\n",
      "---------------\n",
      "\n",
      "Epoch: 11 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.47219704982067107\n",
      "Loss: 0.06080943032911346\n",
      "Loss: 0.033779788647378454\n",
      "Loss: 0.00015826387054709205\n",
      "---------------\n",
      "\n",
      "Epoch: 12 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.4359038863508289\n",
      "Loss: 0.061224744259473975\n",
      "Loss: 0.036415963899855214\n",
      "Loss: 0.0009263740450468118\n",
      "---------------\n",
      "\n",
      "Epoch: 13 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.40321245428334845\n",
      "Loss: 0.061728594400417724\n",
      "Loss: 0.038968650699106606\n",
      "Loss: 0.002206923161900745\n",
      "---------------\n",
      "\n",
      "Epoch: 14 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.37378432804229367\n",
      "Loss: 0.062277660085353855\n",
      "Loss: 0.04141299184633093\n",
      "Loss: 0.003888517182268006\n",
      "---------------\n",
      "\n",
      "Epoch: 15 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.3472874679376185\n",
      "Loss: 0.06284871167565353\n",
      "Loss: 0.0437372429308375\n",
      "Loss: 0.0058806926935713625\n",
      "---------------\n",
      "\n",
      "Epoch: 16 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.3234133648051859\n",
      "Loss: 0.06342856882537269\n",
      "Loss: 0.045936987587075595\n",
      "Loss: 0.00810835737233356\n",
      "---------------\n",
      "\n",
      "Epoch: 17 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.30188244326425817\n",
      "Loss: 0.06400921761102325\n",
      "Loss: 0.048011995018948\n",
      "Loss: 0.010508715872384947\n",
      "---------------\n",
      "\n",
      "Epoch: 18 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.2824443617330212\n",
      "Loss: 0.06458541391431415\n",
      "Loss: 0.049964479213383796\n",
      "Loss: 0.01302924472654293\n",
      "---------------\n",
      "\n",
      "Epoch: 19 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.26487623069428606\n",
      "Loss: 0.06515349129037222\n",
      "Loss: 0.05179810691937157\n",
      "Loss: 0.015626158269002894\n",
      "---------------\n",
      "\n",
      "Epoch: 20 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.24898010827698505\n",
      "Loss: 0.06571075505056848\n",
      "Loss: 0.053517410796977215\n",
      "Loss: 0.018263148126168745\n",
      "---------------\n",
      "\n",
      "Epoch: 21 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.23458037026006398\n",
      "Loss: 0.06625516331241506\n",
      "Loss: 0.055127426728866454\n",
      "Loss: 0.020910308401658736\n",
      "---------------\n",
      "\n",
      "Epoch: 22 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.22152120402948783\n",
      "Loss: 0.06678514981101559\n",
      "Loss: 0.056633459366073474\n",
      "Loss: 0.0235432067568773\n",
      "---------------\n",
      "\n",
      "Epoch: 23 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.20966431894035664\n",
      "Loss: 0.06729951782768667\n",
      "Loss: 0.058040924364345804\n",
      "Loss: 0.02614207903469051\n",
      "---------------\n",
      "\n",
      "Epoch: 24 \n",
      "\n",
      "--------------------\n",
      "Loss: 0.1988868958836713\n",
      "Loss: 0.06779737074053145\n",
      "Loss: 0.05935523893997983\n",
      "Loss: 0.028691131491838988\n"
     ]
    }
   ],
   "source": [
    "models = {}\n",
    "for function, out in outputs.items():\n",
    "    print('------------------------\\n\\n', function, '\\n\\n--------------------------')\n",
    "    model = Perceptron()\n",
    "    model.train(inputs, out, epochs=25, learning_rate=.05)\n",
    "    models[function] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T17:33:47.475125Z",
     "start_time": "2019-09-15T17:33:47.456813Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and\n",
      "[0. 0.] False -0.14306237665075458\n",
      "[0. 1.] False 0.2810046663522525\n",
      "[1. 0.] False 0.31654991759843887\n",
      "[1. 1.] True 0.7406169606014459\n",
      "or\n",
      "[0. 0.] False 0.3247140293650548\n",
      "[0. 1.] True 0.7530823573954817\n",
      "[1. 0.] True 0.7431239285141116\n",
      "[1. 1.] True 1.1714922565445387\n",
      "nor\n",
      "[0. 0.] True 0.5651669640126383\n",
      "[0. 1.] False 0.21781740870969946\n",
      "[1. 0.] False 0.2287803584978617\n",
      "[1. 1.] False -0.11856919680507716\n"
     ]
    }
   ],
   "source": [
    "for function, model in models.items():\n",
    "    print(function)\n",
    "    for inp in inputs:\n",
    "        print(inp, model.predict(inp), model.forward(inp).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
